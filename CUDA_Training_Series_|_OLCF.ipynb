{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqKWzqO68/WaTSgO3FNEiR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik-vala/misc/blob/main/CUDA_Training_Series_%7C_OLCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/olcf/cuda-training-series.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6TaxkJADLyM",
        "outputId": "212b1593-9bfd-405e-be8e-309c31ead337"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-training-series'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 389 (delta 93), reused 83 (delta 83), pack-reused 264 (from 1)\u001b[K\n",
            "Receiving objects: 100% (389/389), 165.83 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (173/173), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y nsight-compute-2023.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM6rqedCXu9M",
        "outputId": "2934a963-0afc-4280-c9b2-e86f6d281c8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,196 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,626 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,516 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,458 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,753 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,475 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,543 kB]\n",
            "Fetched 24.2 MB in 5s (5,125 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  nsight-compute-2023.2.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 721 MB of archives.\n",
            "After this operation, 1,538 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2023.2.0 2023.2.0.16-1 [721 MB]\n",
            "Fetched 721 MB in 27s (26.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nsight-compute-2023.2.0.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack .../nsight-compute-2023.2.0_2023.2.0.16-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2023.2.0 (2023.2.0.16-1) ...\n",
            "Setting up nsight-compute-2023.2.0 (2023.2.0.16-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "-oALLKZICgvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW1"
      ],
      "metadata": {
        "id": "sxEMjefWClsf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2EPU_n9CXir",
        "outputId": "dd69375d-f832-43fc-ad0e-a88c568e6f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Homework 1\n",
            "\n",
            "These exercises will have you write some basic CUDA applications. You will learn how to allocate GPU memory, move data between the host and the GPU, and launch kernels.\n",
            "\n",
            "## **1. Hello World**\n",
            "\n",
            "Your first task is to create a simple hello world application in CUDA. The code skeleton is already given to you in `hello.cu`. Edit that file, paying attention to the FIXME locations, so that the output when run is like this:\n",
            "\n",
            "```\n",
            "Hello from block: 0, thread: 0\n",
            "Hello from block: 0, thread: 1\n",
            "Hello from block: 1, thread: 0\n",
            "Hello from block: 1, thread: 1\n",
            "```\n",
            "\n",
            "(the ordering of the above lines may vary; ordering differences do not indicate an incorrect result)\n",
            "\n",
            "Note the use of `cudaDeviceSynchronize()` after the kernel launch. In CUDA, kernel launches are *asynchronous* to the host thread. The host thread will launch a kernel but not wait for it to finish, before proceeding with the next line of host code. Therefore, to prevent application termination before the kernel gets to print out its message, we must use this synchronization function.\n",
            "\n",
            "After editing the code, compile it using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o hello hello.cu\n",
            "```\n",
            "\n",
            "The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. `nvcc` is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n",
            "\n",
            "If you have trouble, you can look at `hello_solution.cu` for a complete example.\n",
            "\n",
            "To run your code at OLCF on Summit, we will use an LSF command:\n",
            "\n",
            "```\n",
            "bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./hello\n",
            "```\n",
            "\n",
            "Alternatively, you may want to create an alias for your `bsub` command in order to make subsequent runs easier:\n",
            "\n",
            "```\n",
            "alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n",
            "lsfrun ./hello\n",
            "```\n",
            "\n",
            "To run your code at NERSC on Cori, we can use Slurm:\n",
            "\n",
            "```\n",
            "module load esslurm\n",
            "srun -C gpu -N 1 -n 1 -t 10 -A m3502 --gres=gpu:1 -c 10 ./hello\n",
            "```\n",
            "\n",
            "Allocation `m3502` is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance until January 18, 2020. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes, you may use that instead.\n",
            "\n",
            "If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active:\n",
            "\n",
            "```\n",
            "salloc -C gpu -N 1 -t 60 -A m3502 --gres=gpu:1 -c 10\n",
            "srun -n 1 ./hello\n",
            "```\n",
            "\n",
            "Note that you only need to `module load esslurm` once per login session; this is what enables you to submit to the Cori GPU nodes.\n",
            "\n",
            "## **2. Vector Add**\n",
            "\n",
            "If you're up for a challenge, see if you can write a complete vector add program from scratch. Or if you prefer, there is a skeleton code given to you in `vector_add.cu`. Edit the code to build a complete vector_add program. Compile it and run it similar to the method given in exercise 1. You can refer to `vector_add_solution.cu` for a complete example.\n",
            "\n",
            "Note that this skeleton code includes something we didn't cover in lesson 1: CUDA error checking. Every CUDA runtime API call returns an error code. It's good practice (especially if you're having trouble) to rigorously check these error codes. A macro is given that will make this job easier. Note the special error checking method after a kernel call.\n",
            "\n",
            "Typical output when complete would look like this:\n",
            "```\n",
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n",
            "```\n",
            "\n",
            "## **3. Matrix Multiply (naive)**\n",
            "\n",
            "A skeleton naive matrix multiply is given to you in `matrix_mul.cu`. See if you can complete it to get a correct result. If you need help, you can refer to `matrix_mul_solution.cu`.\n",
            "\n",
            "This example introduces 2D threadblock/grid indexing, something we did not cover in lesson 1. If you study the code you will probably be able to see how it is a structural extension from the 1D case.\n",
            "\n",
            "This code includes built-in error checking, so a correct result is indicated by the program.\n"
          ]
        }
      ],
      "source": [
        "!cat cuda-training-series/exercises/hw1/readme.md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Hello World"
      ],
      "metadata": {
        "id": "W8unVuaJF6sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/hello.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYG5C65EDorZ",
        "outputId": "09b2548f-5205-4e69-eb37-4b6d45fde07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "__global__ void hello(){\n",
            "\n",
            "  printf(\"Hello from block: %u, thread: %u\\n\", FIXME);\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  hello<<<FIXME>>>();\n",
            "  cudaDeviceSynchronize();\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw1/hello.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello(){\n",
        "\n",
        "  printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  hello<<<2, 2>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0GX84PrEcF-",
        "outputId": "d8a23b7c-edd5-4c3a-d24f-9cf4a3957a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda-training-series/exercises/hw1/hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw1/hello.cu -o cuda-training-series/exercises/hw1/hello\n",
        "./cuda-training-series/exercises/hw1/hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyDs9ls9E2w-",
        "outputId": "4a56f4b2-0e8b-4fba-f027-c1c56bd9a663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block: 0, thread: 0\n",
            "Hello from block: 0, thread: 1\n",
            "Hello from block: 1, thread: 0\n",
            "Hello from block: 1, thread: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/hello_solution.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "minBEBOCFMtc",
        "outputId": "fe589204-051a-4e3b-f217-cd70b2cfd2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "__global__ void hello(){\n",
            "\n",
            "  printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
            "}\n",
            "\n",
            "int main(){\n",
            "  hello<<<2,2>>>();\n",
            "  cudaDeviceSynchronize();\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Vector Add"
      ],
      "metadata": {
        "id": "OjkX7sSNGACB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/vector_add.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_QRcdXjGcTH",
        "outputId": "1713a50d-cfd1-4ec2-b86e-129af8aadf60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 4096;\n",
            "const int block_size = 256;  // CUDA maximum is 1024\n",
            "// vector add kernel: C = A + B\n",
            "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
            "\n",
            "  int idx = FIXME // create typical 1D thread index from built-in variables\n",
            "  if (idx < ds)\n",
            "    FIXME         // do the vector (element) add here\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n",
            "  h_B = new float[DSIZE];\n",
            "  h_C = new float[DSIZE];\n",
            "  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n",
            "    h_A[i] = rand()/(float)RAND_MAX;\n",
            "    h_B[i] = rand()/(float)RAND_MAX;\n",
            "    h_C[i] = 0;}\n",
            "  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n",
            "  FIXME // allocate device space for vector B\n",
            "  FIXME // allocate device space for vector C\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "  // copy vector A to device:\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  // copy vector B to device:\n",
            "  FIXME\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  vadd<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector C from device to host:\n",
            "  FIXME\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
            "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
            "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw1/vector_add_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// error checking macro\n",
        "#define cudaCheckErrors(msg) \\\n",
        "    do { \\\n",
        "        cudaError_t __err = cudaGetLastError(); \\\n",
        "        if (__err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "                msg, cudaGetErrorString(__err), \\\n",
        "                __FILE__, __LINE__); \\\n",
        "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "\n",
        "const int DSIZE = 4096;\n",
        "const int block_size = 256;  // CUDA maximum is 1024\n",
        "// vector add kernel: C = A + B\n",
        "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
        "\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x; // create typical 1D thread index from built-in variables\n",
        "  if (idx < ds) {\n",
        "    C[idx] = A[idx] + B[idx];\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
        "  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n",
        "  h_B = new float[DSIZE];\n",
        "  h_C = new float[DSIZE];\n",
        "  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n",
        "    h_A[i] = rand()/(float)RAND_MAX;\n",
        "    h_B[i] = rand()/(float)RAND_MAX;\n",
        "    h_C[i] = 0;}\n",
        "  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n",
        "  cudaMalloc(&d_B, DSIZE*sizeof(float)); // allocate device space for vector B\n",
        "  cudaMalloc(&d_C, DSIZE*sizeof(float)); // allocate device space for vector C\n",
        "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
        "  // copy vector A to device:\n",
        "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  // copy vector B to device:\n",
        "  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
        "  //cuda processing sequence step 1 is complete\n",
        "  vadd<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "  //cuda processing sequence step 2 is complete\n",
        "  // copy vector C from device to host:\n",
        "  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  //cuda processing sequence step 3 is complete\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
        "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
        "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gggs92J9GBn-",
        "outputId": "322b3058-5feb-4026-9eda-e3a768640703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda-training-series/exercises/hw1/vector_add_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw1/vector_add_hardik.cu -o cuda-training-series/exercises/hw1/vector_add_hardik\n",
        "./cuda-training-series/exercises/hw1/vector_add_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pc_lU6cHspt",
        "outputId": "df5a219d-64e3-4661-d214-a9842d7b90bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/vector_add_solution.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzFkx4RQI5Se",
        "outputId": "515e11b2-6dee-4d8c-f10a-2000acc7d81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 4096;\n",
            "const int block_size = 256;  // CUDA maximum is 1024\n",
            "// vector add kernel: C = A + B\n",
            "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
            "\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
            "  if (idx < ds)\n",
            "    C[idx] = A[idx] + B[idx];\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "  h_A = new float[DSIZE];\n",
            "  h_B = new float[DSIZE];\n",
            "  h_C = new float[DSIZE];\n",
            "  for (int i = 0; i < DSIZE; i++){\n",
            "    h_A[i] = rand()/(float)RAND_MAX;\n",
            "    h_B[i] = rand()/(float)RAND_MAX;\n",
            "    h_C[i] = 0;}\n",
            "  cudaMalloc(&d_A, DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_B, DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_C, DSIZE*sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMalloc failure\");\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  vadd<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
            "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
            "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Matrix Multiplication (naive)"
      ],
      "metadata": {
        "id": "fdi7qKHSJHQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/matrix_mul.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJph3SFZJNs4",
        "outputId": "de38e625-edb7-41f0-dd8b-d650811745e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// these are just for timing measurments\n",
            "#include <time.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 4096;\n",
            "const int block_size = 16;  // CUDA maximum is 1024 *total* threads in block\n",
            "const float A_val = 1.0f;\n",
            "const float B_val = 2.0f;\n",
            "\n",
            "// matrix multiply (naive) kernel: C = A * B\n",
            "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
            "\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
            "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
            "\n",
            "  if ((idx < ds) && (idy < ds)){\n",
            "    float temp = 0;\n",
            "    for (int i = 0; i < ds; i++)\n",
            "      temp += A[FIXME*ds+i] * B[i*ds+FIXME];   // dot product of row and column\n",
            "    C[idy*ds+idx] = temp;\n",
            "  }\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "\n",
            "  // these are just for timing\n",
            "  clock_t t0, t1, t2;\n",
            "  double t1sum=0.0;\n",
            "  double t2sum=0.0;\n",
            "\n",
            "  // start timing\n",
            "  t0 = clock();\n",
            "\n",
            "  h_A = new float[DSIZE*DSIZE];\n",
            "  h_B = new float[DSIZE*DSIZE];\n",
            "  h_C = new float[DSIZE*DSIZE];\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
            "    h_A[i] = A_val;\n",
            "    h_B[i] = B_val;\n",
            "    h_C[i] = 0;}\n",
            "\n",
            "  // Initialization timing\n",
            "  t1 = clock();\n",
            "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
            "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
            "\n",
            "  // Allocate device memory and copy input data over to GPU\n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMalloc failure\");\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "\n",
            "  // Cuda processing sequence step 1 is complete\n",
            "\n",
            "  // Launch kernel\n",
            "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
            "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
            "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "\n",
            "  // Cuda processing sequence step 2 is complete\n",
            "\n",
            "  // Copy results back to host\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // GPU timing\n",
            "  t2 = clock();\n",
            "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
            "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
            "\n",
            "  // Cuda processing sequence step 3 is complete\n",
            "\n",
            "  // Verify results\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
            "  printf(\"Success!\\n\"); \n",
            "\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw1/matrix_mul_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// these are just for timing measurments\n",
        "#include <time.h>\n",
        "\n",
        "// error checking macro\n",
        "#define cudaCheckErrors(msg) \\\n",
        "    do { \\\n",
        "        cudaError_t __err = cudaGetLastError(); \\\n",
        "        if (__err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "                msg, cudaGetErrorString(__err), \\\n",
        "                __FILE__, __LINE__); \\\n",
        "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "\n",
        "const int DSIZE = 4096;\n",
        "const int block_size = 16;  // CUDA maximum is 1024 *total* threads in block\n",
        "const float A_val = 1.0f;\n",
        "const float B_val = 2.0f;\n",
        "\n",
        "// matrix multiply (naive) kernel: C = A * B\n",
        "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
        "\n",
        "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
        "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
        "\n",
        "  if ((idx < ds) && (idy < ds)){\n",
        "    float temp = 0;\n",
        "    for (int i = 0; i < ds; i++)\n",
        "      temp += A[idy*ds+i] * B[i*ds+idx];   // dot product of row and column\n",
        "    C[idy*ds+idx] = temp;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
        "\n",
        "  // these are just for timing\n",
        "  clock_t t0, t1, t2;\n",
        "  double t1sum=0.0;\n",
        "  double t2sum=0.0;\n",
        "\n",
        "  // start timing\n",
        "  t0 = clock();\n",
        "\n",
        "  h_A = new float[DSIZE*DSIZE];\n",
        "  h_B = new float[DSIZE*DSIZE];\n",
        "  h_C = new float[DSIZE*DSIZE];\n",
        "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
        "    h_A[i] = A_val;\n",
        "    h_B[i] = B_val;\n",
        "    h_C[i] = 0;}\n",
        "\n",
        "  // Initialization timing\n",
        "  t1 = clock();\n",
        "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
        "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
        "\n",
        "  // Allocate device memory and copy input data over to GPU\n",
        "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaCheckErrors(\"cudaMalloc failure\");\n",
        "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
        "\n",
        "  // Cuda processing sequence step 1 is complete\n",
        "\n",
        "  // Launch kernel\n",
        "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
        "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
        "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "\n",
        "  // Cuda processing sequence step 2 is complete\n",
        "\n",
        "  // Copy results back to host\n",
        "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // GPU timing\n",
        "  t2 = clock();\n",
        "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
        "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
        "\n",
        "  // Cuda processing sequence step 3 is complete\n",
        "\n",
        "  // Verify results\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
        "  printf(\"Success!\\n\");\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ygm5PiIJYZk",
        "outputId": "b7c8f2e9-633a-4e48-a1e1-77dbfd7faa58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda-training-series/exercises/hw1/matrix_mul_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw1/matrix_mul_hardik.cu -o cuda-training-series/exercises/hw1/matrix_mul_hardik\n",
        "./cuda-training-series/exercises/hw1/matrix_mul_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l58pw3aSLZO_",
        "outputId": "93927c35-e9e4-4664-99c8-5f595682d9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init took 0.150067 seconds.  Begin compute\n",
            "Done. Compute took 1.652004 seconds\n",
            "Success!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw1/matrix_mul_solution.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZe51KIDLis8",
        "outputId": "b76b3596-d6a0-4bfc-e320-d096c8a50125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// these are just for timing measurments\n",
            "#include <time.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 8192;\n",
            "const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n",
            "const float A_val = 3.0f;\n",
            "const float B_val = 2.0f;\n",
            "\n",
            "// matrix multiply (naive) kernel: C = A * B\n",
            "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
            "\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
            "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
            "\n",
            "  if ((idx < ds) && (idy < ds)){\n",
            "    float temp = 0;\n",
            "    for (int i = 0; i < ds; i++)\n",
            "      temp += A[idy*ds+i] * B[i*ds+idx];   // dot product of row and column\n",
            "    C[idy*ds+idx] = temp;\n",
            "  }\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "\n",
            "\n",
            "  // these are just for timing\n",
            "  clock_t t0, t1, t2;\n",
            "  double t1sum=0.0;\n",
            "  double t2sum=0.0;\n",
            "\n",
            "  // start timing\n",
            "  t0 = clock();\n",
            "\n",
            "  h_A = new float[DSIZE*DSIZE];\n",
            "  h_B = new float[DSIZE*DSIZE];\n",
            "  h_C = new float[DSIZE*DSIZE];\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
            "    h_A[i] = A_val;\n",
            "    h_B[i] = B_val;\n",
            "    h_C[i] = 0;}\n",
            "\n",
            "  // Initialization timing\n",
            "  t1 = clock();\n",
            "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
            "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
            "\n",
            "  // Allocate device memory and copy input data over to GPU\n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMalloc failure\");\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "\n",
            "  // Cuda processing sequence step 1 is complete\n",
            "\n",
            "  // Launch kernel\n",
            "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
            "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
            "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "\n",
            "  // Cuda processing sequence step 2 is complete\n",
            "\n",
            "  // Copy results back to host\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // GPU timing\n",
            "  t2 = clock();\n",
            "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
            "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
            "\n",
            "  // Cuda processing sequence step 3 is complete\n",
            "\n",
            "  // Verify results\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
            "  printf(\"Success!\\n\"); \n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW2"
      ],
      "metadata": {
        "id": "usJDUzoNT0kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw2/readme.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkh2NC1NT6pp",
        "outputId": "84dff643-5ffa-425b-996b-3142b8936867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Homework 2\n",
            "\n",
            "These exercises will help reinforce the concept of Shared Memory on the GPU.\n",
            "\n",
            "## **1. 1D Stencil Using Shared Memory**\n",
            "\n",
            "Your first task is to create a 1D stencil application that uses shared memory. The code skeleton is provided in *stencil_1d.cu*. Edit that file, paying attention to the FIXME locations. The code will verify output and report any errors.\n",
            "\n",
            "After editing the code, compile it using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o stencil_1d stencil_1d.cu\n",
            "```\n",
            "\n",
            "The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. *nvcc* is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n",
            "\n",
            "To run your code, we will use an LSF command:\n",
            "\n",
            "```\n",
            "bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./stencil_1d\n",
            "```\n",
            "\n",
            "Alternatively, you may want to create an alias for your *bsub* command in order to make subsequent runs easier:\n",
            "\n",
            "```\n",
            "alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n",
            "lsfrun ./stencil_1d\n",
            "```\n",
            "\n",
            "To run your code at NERSC on Cori, we can use Slurm:\n",
            "\n",
            "```\n",
            "module load esslurm\n",
            "srun -C gpu -N 1 -n 1 -t 10 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10 ./stencil_1d\n",
            "```\n",
            "\n",
            "Allocation `m3502` is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes (such as m1759), you may use that instead.\n",
            "\n",
            "If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active (this is recommended if there are enough available nodes):\n",
            "\n",
            "```\n",
            "salloc -C gpu -N 1 -t 60 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10\n",
            "srun -n 1 ./stencil_1d\n",
            "```\n",
            "\n",
            "Note that you only need to `module load esslurm` once per login session; this is what enables you to submit to the Cori GPU nodes.\n",
            "\n",
            "If you have trouble, you can look at *stencil_1d_solution* for a complete example.\n",
            "\n",
            "## **2. 2D Matrix Multiply Using Shared Memory**\n",
            "\n",
            "Next, let's apply shared memory to the 2D matrix multiply we wrote in Homework 1. FIXME locations are provided in the code skeleton in *matrix_mul_shared.cu*. See if you can successfully load the required data into shared memory and then appropriately update the dot product calculation. Compile and run your code using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o matrix_mul matrix_mul_shared.cu\n",
            "lsfrun ./matrix_mul\n",
            "```\n",
            "\n",
            "Note that timing information is included. Go back and run your solution from Homework 1 and observe the runtime. What runtime impact do you notice after applying shared memory to this 2D matrix multiply? How does it differ from the runtime you observed in your previous implementation?\n",
            "\n",
            "If you have trouble, you can look at *matrix_mul_shared_solution* for a complete example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 1D Stencil Using Shared Memory"
      ],
      "metadata": {
        "id": "b_zrN_qjUQx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw2/stencil_1d.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmbURPiVUwsD",
        "outputId": "0260d09b-8099-454b-a9a7-36b461886389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "#define N 4096\n",
            "#define RADIUS 3\n",
            "#define BLOCK_SIZE 16\n",
            "\n",
            "__global__ void stencil_1d(int *in, int *out) {\n",
            "    __shared__ int temp[FIXME];\n",
            "    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n",
            "    int lindex = FIXME;\n",
            "\n",
            "    // Read input elements into shared memory\n",
            "    temp[lindex] = in[gindex];\n",
            "    if (threadIdx.x < RADIUS) {\n",
            "      temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
            "      temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
            "    }\n",
            "\n",
            "    // Synchronize (ensure all the data is available)\n",
            "    __syncthreads();\n",
            "\n",
            "    // Apply the stencil\n",
            "    int result = 0;\n",
            "    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n",
            "      result += temp[FIXME];\n",
            "\n",
            "    // Store the result\n",
            "    out[gindex] = result;\n",
            "}\n",
            "\n",
            "void fill_ints(int *x, int n) {\n",
            "  fill_n(x, n, 1);\n",
            "}\n",
            "\n",
            "int main(void) {\n",
            "  int *in, *out; // host copies of a, b, c\n",
            "  int *d_in, *d_out; // device copies of a, b, c\n",
            "\n",
            "  // Alloc space for host copies and setup values\n",
            "  int size = (FIXME) * sizeof(int);\n",
            "  in = (int *)malloc(size); fill_ints(in, N + 2*RADIUS);\n",
            "  out = (int *)malloc(size); fill_ints(out, N + 2*RADIUS);\n",
            "\n",
            "  // Alloc space for device copies\n",
            "  cudaMalloc((void **)&d_in, size);\n",
            "  cudaMalloc((void **)&d_out, size);\n",
            "\n",
            "  // Copy to device\n",
            "  cudaMemcpy(d_in, in, size, cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_out, out, size, cudaMemcpyHostToDevice);\n",
            "\n",
            "  // Launch stencil_1d() kernel on GPU\n",
            "  stencil_1d<<<N/BLOCK_SIZE,BLOCK_SIZE>>>(FIXME, FIXME);\n",
            "\n",
            "  // Copy result back to host\n",
            "  cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // Error Checking\n",
            "  for (int i = 0; i < N + 2*RADIUS; i++) {\n",
            "    if (i<RADIUS || i>=N+RADIUS){\n",
            "      if (out[i] != 1)\n",
            "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1);\n",
            "    } else {\n",
            "      if (out[i] != 1 + 2*RADIUS)\n",
            "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1 + 2*RADIUS);\n",
            "    }\n",
            "  }\n",
            "\n",
            "  // Cleanup\n",
            "  free(in); free(out);\n",
            "  cudaFree(d_in); cudaFree(d_out);\n",
            "  printf(\"Success!\\n\");\n",
            "  return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw2/stencil_1d_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <algorithm>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define N 4096\n",
        "#define RADIUS 3\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) {\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) {\n",
        "      temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "      temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Synchronize (ensure all the data is available)\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n",
        "      result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex] = result;\n",
        "}\n",
        "\n",
        "void fill_ints(int *x, int n) {\n",
        "  fill_n(x, n, 1);\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  int *in, *out; // host copies of a, b, c\n",
        "  int *d_in, *d_out; // device copies of a, b, c\n",
        "\n",
        "  // Alloc space for host copies and setup values\n",
        "  int size = (N + 2*RADIUS) * sizeof(int);\n",
        "  in = (int *)malloc(size); fill_ints(in, N + 2*RADIUS);\n",
        "  out = (int *)malloc(size); fill_ints(out, N + 2*RADIUS);\n",
        "\n",
        "  // Alloc space for device copies\n",
        "  cudaMalloc((void **)&d_in, size);\n",
        "  cudaMalloc((void **)&d_out, size);\n",
        "\n",
        "  // Copy to device\n",
        "  cudaMemcpy(d_in, in, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_out, out, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Launch stencil_1d() kernel on GPU\n",
        "  stencil_1d<<<N/BLOCK_SIZE,BLOCK_SIZE>>>(d_in + RADIUS, d_out + RADIUS);\n",
        "\n",
        "  // Copy result back to host\n",
        "  cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Error Checking\n",
        "  for (int i = 0; i < N + 2*RADIUS; i++) {\n",
        "    if (i<RADIUS || i>=N+RADIUS){\n",
        "      if (out[i] != 1)\n",
        "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1);\n",
        "    } else {\n",
        "      if (out[i] != 1 + 2*RADIUS)\n",
        "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1 + 2*RADIUS);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Cleanup\n",
        "  free(in); free(out);\n",
        "  cudaFree(d_in); cudaFree(d_out);\n",
        "  printf(\"Success!\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHX8YpVkU5W9",
        "outputId": "3f0cb643-9de4-4201-c6f0-8e049284bcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda-training-series/exercises/hw2/stencil_1d_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw2/stencil_1d_hardik.cu -o cuda-training-series/exercises/hw2/stencil_1d_hardik\n",
        "./cuda-training-series/exercises/hw2/stencil_1d_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw8AfPX7U7R6",
        "outputId": "540cd220-b529-4d14-e680-e755ec1e1a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw2/stencil_1d_solution.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DufZUhSuVSuJ",
        "outputId": "c8384662-600e-44d8-9b71-2b7b62fe8ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "#define N 4096\n",
            "#define RADIUS 3\n",
            "#define BLOCK_SIZE 16\n",
            "\n",
            "__global__ void stencil_1d(int *in, int *out) {\n",
            "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
            "    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n",
            "    int lindex = threadIdx.x + RADIUS;\n",
            "\n",
            "    // Read input elements into shared memory\n",
            "    temp[lindex] = in[gindex];\n",
            "    if (threadIdx.x < RADIUS) {\n",
            "      temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
            "      temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
            "    }\n",
            "\n",
            "    // Synchronize (ensure all the data is available)\n",
            "    __syncthreads();\n",
            "\n",
            "    // Apply the stencil\n",
            "    int result = 0;\n",
            "    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n",
            "      result += temp[lindex + offset];\n",
            "\n",
            "    // Store the result\n",
            "    out[gindex] = result;\n",
            "}\n",
            "\n",
            "void fill_ints(int *x, int n) {\n",
            "  fill_n(x, n, 1);\n",
            "}\n",
            "\n",
            "int main(void) {\n",
            "  int *in, *out; // host copies of a, b, c\n",
            "  int *d_in, *d_out; // device copies of a, b, c\n",
            "  int size = (N + 2*RADIUS) * sizeof(int);\n",
            "\n",
            "  // Alloc space for host copies and setup values\n",
            "  in = (int *)malloc(size); fill_ints(in, N + 2*RADIUS);\n",
            "  out = (int *)malloc(size); fill_ints(out, N + 2*RADIUS);\n",
            "\n",
            "  // Alloc space for device copies\n",
            "  cudaMalloc((void **)&d_in, size);\n",
            "  cudaMalloc((void **)&d_out, size);\n",
            "\n",
            "  // Copy to device\n",
            "  cudaMemcpy(d_in, in, size, cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_out, out, size, cudaMemcpyHostToDevice);\n",
            "\n",
            "  // Launch stencil_1d() kernel on GPU\n",
            "  stencil_1d<<<N/BLOCK_SIZE,BLOCK_SIZE>>>(d_in + RADIUS, d_out + RADIUS);\n",
            "\n",
            "  // Copy result back to host\n",
            "  cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // Error Checking\n",
            "  for (int i = 0; i < N + 2*RADIUS; i++) {\n",
            "    if (i<RADIUS || i>=N+RADIUS){\n",
            "      if (out[i] != 1)\n",
            "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1);\n",
            "    } else {\n",
            "      if (out[i] != 1 + 2*RADIUS)\n",
            "    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1 + 2*RADIUS);\n",
            "    }\n",
            "  }\n",
            "\n",
            "  // Cleanup\n",
            "  free(in); free(out);\n",
            "  cudaFree(d_in); cudaFree(d_out);\n",
            "  printf(\"Success!\\n\");\n",
            "  return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 2D Matrix Multiply Using Shared Memory"
      ],
      "metadata": {
        "id": "OXtZ_MK-UTC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw2/matrix_mul_shared.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DlplqW_VZmB",
        "outputId": "31cc5e75-be87-4a42-95a2-141736773707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// these are just for timing measurments\n",
            "#include <time.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 8192;\n",
            "const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n",
            "const float A_val = 3.0f;\n",
            "const float B_val = 2.0f;\n",
            "\n",
            "// matrix multiply (naive) kernel: C = A * B\n",
            "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
            "\n",
            "  // declare cache in shared memory\n",
            "  __shared__ float As[block_size][block_size];\n",
            "  __shared__ float Bs[block_size][block_size];\n",
            "\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
            "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
            "\n",
            "  if ((idx < ds) && (idy < ds)){\n",
            "    float temp = 0;\n",
            "    for (int i = 0; i < ds/block_size; i++) {\n",
            "\n",
            "      // Load data into shared memory\n",
            "      As[threadIdx.y][threadIdx.x] = A[FIXME];\n",
            "      Bs[threadIdx.y][threadIdx.x] = B[FIXME];\n",
            "\n",
            "      // Synchronize\n",
            "      __syncthreads();\n",
            "\n",
            "      // Keep track of the running sum\n",
            "      for (int k = 0; k < block_size; k++)\n",
            "      \ttemp += As[FIXME][FIXME] * Bs[FIXME][FIXME]; // dot product of row and column\n",
            "      __syncthreads();\n",
            "\n",
            "    }\n",
            "\n",
            "    // Write to global memory\n",
            "    C[idy*ds+idx] = temp;\n",
            "  }\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "\n",
            "\n",
            "  // these are just for timing\n",
            "  clock_t t0, t1, t2;\n",
            "  double t1sum=0.0;\n",
            "  double t2sum=0.0;\n",
            "\n",
            "  // start timing\n",
            "  t0 = clock();\n",
            "\n",
            "  h_A = new float[DSIZE*DSIZE];\n",
            "  h_B = new float[DSIZE*DSIZE];\n",
            "  h_C = new float[DSIZE*DSIZE];\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
            "    h_A[i] = A_val;\n",
            "    h_B[i] = B_val;\n",
            "    h_C[i] = 0;}\n",
            "\n",
            "  // Initialization timing\n",
            "  t1 = clock();\n",
            "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
            "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
            "\n",
            "  // Allocate device memory and copy input data over to GPU\n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMalloc failure\");\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "\n",
            "  // Cuda processing sequence step 1 is complete\n",
            "\n",
            "  // Launch kernel\n",
            "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
            "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
            "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "\n",
            "  // Cuda processing sequence step 2 is complete\n",
            "\n",
            "  // Copy results back to host\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // GPU timing\n",
            "  t2 = clock();\n",
            "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
            "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
            "\n",
            "  // Cuda processing sequence step 3 is complete\n",
            "\n",
            "  // Verify results\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
            "  printf(\"Success!\\n\"); \n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw2/matrix_mul_shared_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// these are just for timing measurments\n",
        "#include <time.h>\n",
        "\n",
        "// error checking macro\n",
        "#define cudaCheckErrors(msg) \\\n",
        "    do { \\\n",
        "        cudaError_t __err = cudaGetLastError(); \\\n",
        "        if (__err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "                msg, cudaGetErrorString(__err), \\\n",
        "                __FILE__, __LINE__); \\\n",
        "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "\n",
        "const int DSIZE = 8192;\n",
        "const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n",
        "const float A_val = 3.0f;\n",
        "const float B_val = 2.0f;\n",
        "\n",
        "// matrix multiply (naive) kernel: C = A * B\n",
        "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
        "\n",
        "  // declare cache in shared memory\n",
        "  __shared__ float As[block_size][block_size];\n",
        "  __shared__ float Bs[block_size][block_size];\n",
        "\n",
        "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
        "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
        "\n",
        "  if ((idx < ds) && (idy < ds)){\n",
        "    float temp = 0;\n",
        "    for (int i = 0; i < ds/block_size; i++) {\n",
        "\n",
        "      // Load data into shared memory\n",
        "      As[threadIdx.y][threadIdx.x] = A[idy * ds + i * block_size + threadIdx.x];\n",
        "      Bs[threadIdx.y][threadIdx.x] = B[(i * block_size + threadIdx.y) * ds + idx];\n",
        "\n",
        "      // Synchronize\n",
        "      __syncthreads();\n",
        "\n",
        "      // Keep track of the running sum\n",
        "      for (int k = 0; k < block_size; k++)\n",
        "      \ttemp += As[threadIdx.y][k] * Bs[k][threadIdx.x]; // dot product of row and column\n",
        "      __syncthreads();\n",
        "\n",
        "    }\n",
        "\n",
        "    // Write to global memory\n",
        "    C[idy*ds+idx] = temp;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
        "\n",
        "\n",
        "  // these are just for timing\n",
        "  clock_t t0, t1, t2;\n",
        "  double t1sum=0.0;\n",
        "  double t2sum=0.0;\n",
        "\n",
        "  // start timing\n",
        "  t0 = clock();\n",
        "\n",
        "  h_A = new float[DSIZE*DSIZE];\n",
        "  h_B = new float[DSIZE*DSIZE];\n",
        "  h_C = new float[DSIZE*DSIZE];\n",
        "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
        "    h_A[i] = A_val;\n",
        "    h_B[i] = B_val;\n",
        "    h_C[i] = 0;}\n",
        "\n",
        "  // Initialization timing\n",
        "  t1 = clock();\n",
        "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
        "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
        "\n",
        "  // Allocate device memory and copy input data over to GPU\n",
        "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
        "  cudaCheckErrors(\"cudaMalloc failure\");\n",
        "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
        "\n",
        "  // Cuda processing sequence step 1 is complete\n",
        "\n",
        "  // Launch kernel\n",
        "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
        "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
        "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "\n",
        "  // Cuda processing sequence step 2 is complete\n",
        "\n",
        "  // Copy results back to host\n",
        "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // GPU timing\n",
        "  t2 = clock();\n",
        "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
        "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
        "\n",
        "  // Cuda processing sequence step 3 is complete\n",
        "\n",
        "  // Verify results\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
        "  printf(\"Success!\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmBVStcbVbFa",
        "outputId": "a3347ce9-cfde-45ce-a776-1629cecdd7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda-training-series/exercises/hw2/matrix_mul_shared_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw2/matrix_mul_shared_hardik.cu -o cuda-training-series/exercises/hw2/matrix_mul_shared_hardik\n",
        "./cuda-training-series/exercises/hw2/matrix_mul_shared_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luG2RdzPVbQF",
        "outputId": "d24f16e6-cf40-4f6b-c66e-7d5874fdc955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init took 0.554319 seconds.  Begin compute\n",
            "Done. Compute took 1.508787 seconds\n",
            "Success!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw2/matrix_mul_shared_solution.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SohESQ0XVbZd",
        "outputId": "5098fed0-c0e4-4b77-b61d-d186bac40e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// these are just for timing measurments\n",
            "#include <time.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 8192;\n",
            "const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n",
            "const float A_val = 3.0f;\n",
            "const float B_val = 2.0f;\n",
            "\n",
            "// matrix multiply (naive) kernel: C = A * B\n",
            "__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n",
            "\n",
            "  // declare cache in shared memory\n",
            "  __shared__ float As[block_size][block_size];\n",
            "  __shared__ float Bs[block_size][block_size];\n",
            "\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n",
            "  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n",
            "\n",
            "  if ((idx < ds) && (idy < ds)){\n",
            "    float temp = 0;\n",
            "    for (int i = 0; i < ds/block_size; i++) {\n",
            "\n",
            "      // Load data into shared memory\n",
            "      As[threadIdx.y][threadIdx.x] = A[idy * ds + (i * block_size + threadIdx.x)];\n",
            "      Bs[threadIdx.y][threadIdx.x] = B[(i * block_size + threadIdx.y) * ds + idx];\n",
            "\n",
            "      // Synchronize\n",
            "      __syncthreads();\n",
            "\n",
            "      // Keep track of the running sum\n",
            "      for (int k = 0; k < block_size; k++)\n",
            "      \ttemp += As[threadIdx.y][k] * Bs[k][threadIdx.x]; // dot product of row and column\n",
            "      __syncthreads();\n",
            "\n",
            "    }\n",
            "\n",
            "    // Write to global memory\n",
            "    C[idy*ds+idx] = temp;\n",
            "  }\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "\n",
            "\n",
            "  // these are just for timing\n",
            "  clock_t t0, t1, t2;\n",
            "  double t1sum=0.0;\n",
            "  double t2sum=0.0;\n",
            "\n",
            "  // start timing\n",
            "  t0 = clock();\n",
            "\n",
            "  h_A = new float[DSIZE*DSIZE];\n",
            "  h_B = new float[DSIZE*DSIZE];\n",
            "  h_C = new float[DSIZE*DSIZE];\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++){\n",
            "    h_A[i] = A_val;\n",
            "    h_B[i] = B_val;\n",
            "    h_C[i] = 0;}\n",
            "\n",
            "  // Initialization timing\n",
            "  t1 = clock();\n",
            "  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n",
            "  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n",
            "\n",
            "  // Allocate device memory and copy input data over to GPU\n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMalloc failure\");\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "\n",
            "  // Cuda processing sequence step 1 is complete\n",
            "\n",
            "  // Launch kernel\n",
            "  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n",
            "  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n",
            "  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "\n",
            "  // Cuda processing sequence step 2 is complete\n",
            "\n",
            "  // Copy results back to host\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "\n",
            "  // GPU timing\n",
            "  t2 = clock();\n",
            "  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n",
            "  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n",
            "\n",
            "  // Cuda processing sequence step 3 is complete\n",
            "\n",
            "  // Verify results\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n",
            "  printf(\"Success!\\n\"); \n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW3"
      ],
      "metadata": {
        "id": "Xk94QugQK6n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw3/readme.md"
      ],
      "metadata": {
        "id": "MEp870RpLAFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b58984-57dd-4019-e960-660abb0f77d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## **1. Vector Add**\n",
            "\n",
            "We'll use a slight variation on the vector add code presented in a previous homework (*vector_add.cu*).  Edit the code to build a complete vector_add program. You can refer to *vector_add_solution.cu* for a complete example.  For this example, we have made a change to the kernel to use something called a grid-stride loop.  This topic will be dealt with in more detail in a later training session, but for now we can describe it as a flexible kernel design method that allows a simple kernel to handle an arbitrary size data set with an arbitrary size \"grid\", i.e. the configuration of blocks and threads associated with the kernel launch.  If you'd like to read more about grid-stride loops right now, you can visit https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/\n",
            "\n",
            "As we will see, this flexibility is important for our investigations in section 2 of this homework session.  However, as before, all you need to focus on are the FIXME items, and these sections will be identical to the work you did in a previous homework assignment.  If you get stuck, you can refer to the solution *vector_add_solution.cu*.\n",
            "\n",
            "Note that this skeleton code includes something we didn't cover in lesson 1: CUDA error checking.  Every CUDA runtime API call returns an error code.  It's good practice (especially if you're having trouble) to rigorously check these error codes.  A macro is given that will make this job easier.  Note the special error checking method after a kernel call.\n",
            "\n",
            "After editing the code, compile it using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o vector_add vector_add.cu\n",
            "```\n",
            "\n",
            "The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. *nvcc* is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n",
            "\n",
            "To run your code, we will use an LSF command:\n",
            "\n",
            "```\n",
            "bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./vector_add\n",
            "```\n",
            "\n",
            "Alternatively, you may want to create an alias for your bsub command in order to make subsequent runs easier:\n",
            "\n",
            "```\n",
            "alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n",
            "lsfrun ./vector_add\n",
            "```\n",
            "\n",
            "To run your code at NERSC on Cori, we can use Slurm:\n",
            "\n",
            "```\n",
            "module load esslurm\n",
            "srun -C gpu -N 1 -n 1 -t 10 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10 ./vector_add\n",
            "```\n",
            "\n",
            "Allocation `m3502` is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes (such as m1759), you may use that instead.\n",
            "\n",
            "If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active (this is recommended if there are enough available nodes):\n",
            "\n",
            "```\n",
            "salloc -C gpu -N 1 -t 60 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10\n",
            "srun -n 1 ./vector_add\n",
            "```\n",
            "\n",
            "Note that you only need to `module load esslurm` once per login session; this is what enables you to submit to the Cori GPU nodes.\n",
            "\n",
            "\n",
            "We've also changed the problem size from the previous example, so correct output should look like this:\n",
            "\n",
            "```\n",
            "A[0] = 0.120663\n",
            "B[0] = 0.615704\n",
            "C[0] = 0.736367\n",
            "```\n",
            "\n",
            "the actual numerical values aren't too important, as long as C[0] = A[0] + B[0]\n",
            "\n",
            "## **2. Profiling Experiments**\n",
            "\n",
            "Our objective now will be to explore some of the concepts we learned in the lesson.  In particular we want to see what effect grid sizing (choice of blocks, and threads per block) have on performance.  We could do analysis like this using host-code-based timing methods, but we'll introduce a new concept, using a GPU profiler.  In a future session, you'll learn more about the GPU profilers (Nsight Compute and Nsight Systems), but for now we will use Nsight Compute in a fairly simple fashion to get some basic data about kernel behavior, to use for comparison.\n",
            "(If you'd like to read more about the Nsight profilers, you can start here: https://devblogs.nvidia.com/migrating-nvidia-nsight-tools-nvvp-nvprof/)\n",
            "\n",
            "First, note that the code has these two lines in it:\n",
            "\n",
            "```\n",
            "  int blocks = 1;  // modify this line for experimentation\n",
            "  int threads = 1; // modify this line for experimentation\n",
            "```\n",
            "\n",
            "These lines control the grid sizing.  The first variable blocks chooses the total number of blocks to launch.  The second variable threads chooses the number of threads per block to launch.  This second variable must be constrained to choices between 1 and 1024, inclusive.  These are limits imposed by the GPU hardware.\n",
            "\n",
            "Let's consider 3 cases.  In each case, we will modify the blocks and threads variables, recompile the code, and then run the code under the Nsight Compute profiler.\n",
            "\n",
            "Nsight Compute is installed as part of newer CUDA toolkits (10.1 and newer), but the path to the command line tool may or may not be set up as part of your CUDA install.  Therefore it may  be necessary to specify the complete command line to access the tool.  We will demonstrate that here with our invocations.\n",
            "\n",
            "For the following profiler experiments, we will assume you have loaded the profile module and acquired a node for interactive usage:\n",
            "\n",
            "```\n",
            "module load nsight-compute\n",
            "bsub -W 30 -nnodes 1 -P <allocation_ID> -Is /bin/bash\n",
            "```\n",
            "\n",
            "### **2a.  1 block of 1 thread**\n",
            "\n",
            "For this experiment, leave the code as you have created it to complete exercise 1 above.  When running the code you may have noticed it takes a few seconds to run, however the duration is not particularly long.  This raises the question \"how much of that time is the kernel running?\"  The profiler can help us answer that question, and we can use this duration (or various other characteristics) as indicators of \"performance\" for comparison.  The kernel is designed to do the same set of arithmetic calculations regardless of the grid sizing choices, so we can say that shorter kernel duration corresponds to higher performance.\n",
            "\n",
            "If you'd like to get a basic idea of \"typical\" profiler output, you could use the following command:\n",
            "\n",
            "```\n",
            "jsrun -n1 -a1 -c1 -g1 nv-nsight-cu-cli ./vector_add\n",
            "```\n",
            "\n",
            "However for this 1 block/1 thread test case, the profiler will spend several minutes assembling the requested set of information.  Since our focus is on kernel duration, we can use a command that allows the profiler to run more quickly:\n",
            "\n",
            "```\n",
            "jsrun -n1 -a1 -c1 -g1 nv-nsight-cu-cli  --section SpeedOfLight --section MemoryWorkloadAnalysis ./vector_add\n",
            "```\n",
            "\n",
            "This will allow the profiler to complete its work in under a minute.\n",
            "\n",
            "We won't parse all the output, but we're interested in these lines:\n",
            "\n",
            "```\n",
            "Duration                                                                        second                           2.86\n",
            "```\n",
            "\n",
            "and:\n",
            "\n",
            "```\n",
            "Memory Throughput                                                         Mbyte/second                         204.25\n",
            "```\n",
            "\n",
            "The above indicate that our kernel took about 3 seconds to run and achieved around 200MB/s \"throughput\" i.e. combined read and write activity, to the GPU memory.  A Tesla V100 has around 700-900 GB/s of available memory throughput, so this code isn't using the available memory bandwidth very well, amongst other issues.  Can we improve the situation with some changes to our grid sizing?\n",
            "\n",
            "### **2b.  1 block of 1024 threads**\n",
            "\n",
            "In our training session, we learned that we want \"lots of threads\".  More specifically we learned that we'd like to deposit as many as 2048 threads on a single SM, and ideally we'd like to do this across all the SMs in the GPU.  This allows the GPU to do \"latency hiding\" which we said was very important for GPU performance, and in the case of this code, the extra thread behavior will help with memory utilization, as well, as we shall see.  In fact, for this code, \"lots of threads/latency hiding\" and \"efficient use of memory\" are two sides of the same coin.  This will become more evident in the next training session.\n",
            "\n",
            "So let's take a baby step with our code.  Let's change from 1 block of 1 thread to 1 block of 1024 threads. As we've learned, this structure isn't very good, because it can use at most a single SM on our GPU, but can it improve performance at all?\n",
            "\n",
            "Edit the code to make the changes to the threads (1024) variable only.  Leave the blocks variable at 1. Recompile the code and then rerun the same profiler command.  What are the kernel duration and (achieved) memory throughput now?\n",
            "\n",
            "(You should now observe a kernel duration that drops from the second range to the millisecond range, and the memory throughput should now be in the GB/s instead of MB/s)\n",
            "\n",
            "### **2c. 160 blocks of 1024 threads**\n",
            "\n",
            "Let's fill the GPU now.  We learned that a Tesla V100 has 80 SMs, and each SM can handle at most 2048 threads.  If we create a grid of 160 blocks, each of 1024 threads, this should allow for maximum \"occupancy\" of our kernel/grid on the GPU.  Make the necessary changes to the blocks (= 160) variable (the threads variable should already be at 1024 from step 2b), recompile the code, and rerun the profiler command as given in 2a.  What is the performance (kernel duration) and achieved memory throughput now?\n",
            "\n",
            "(You should now observe a kernel duration that has dropped to the microsecond range - ~500us  - and a memory throughput that should be \"close\" to the peak theoretical of 900GB/s for a Tesla V100).\n",
            "\n",
            "For the Tesla V100 GPU, this calculation of 80 SMs * 2048 threads/SM = 164K threads is our definition of \"lots of threads\". \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Vector Add"
      ],
      "metadata": {
        "id": "2h3z8LXoLLMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw3/vector_add.cu"
      ],
      "metadata": {
        "id": "ekp3IYr4LPd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20062c46-6d08-4de2-bc7e-47f3040ed101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 32*1048576;\n",
            "// vector add kernel: C = A + B\n",
            "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
            "\n",
            "  for (int idx = threadIdx.x+blockDim.x*blockIdx.x; idx < ds; idx+=gridDim.x*blockDim.x)         // a grid-stride loop\n",
            "    FIXME         // do the vector (element) add here\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n",
            "  h_B = new float[DSIZE];\n",
            "  h_C = new float[DSIZE];\n",
            "  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n",
            "    h_A[i] = rand()/(float)RAND_MAX;\n",
            "    h_B[i] = rand()/(float)RAND_MAX;\n",
            "    h_C[i] = 0;}\n",
            "  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n",
            "  FIXME // allocate device space for vector B\n",
            "  FIXME // allocate device space for vector C\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "  // copy vector A to device:\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  // copy vector B to device:\n",
            "  FIXME\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  int blocks = 1;  // modify this line for experimentation\n",
            "  int threads = 1; // modify this line for experimentation\n",
            "  vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector C from device to host:\n",
            "  FIXME\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
            "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
            "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
            "  return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw3/vector_add_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// error checking macro\n",
        "#define cudaCheckErrors(msg) \\\n",
        "    do { \\\n",
        "        cudaError_t __err = cudaGetLastError(); \\\n",
        "        if (__err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "                msg, cudaGetErrorString(__err), \\\n",
        "                __FILE__, __LINE__); \\\n",
        "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "\n",
        "const int DSIZE = 32*1048576;\n",
        "// vector add kernel: C = A + B\n",
        "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
        "\n",
        "  for (int idx = threadIdx.x+blockDim.x*blockIdx.x; idx < ds; idx+=gridDim.x*blockDim.x)         // a grid-stride loop\n",
        "    C[idx] = A[idx] + B[idx];         // do the vector (element) add here\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
        "  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n",
        "  h_B = new float[DSIZE];\n",
        "  h_C = new float[DSIZE];\n",
        "  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n",
        "    h_A[i] = rand()/(float)RAND_MAX;\n",
        "    h_B[i] = rand()/(float)RAND_MAX;\n",
        "    h_C[i] = 0;}\n",
        "  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n",
        "  cudaMalloc(&d_B, DSIZE*sizeof(float));  // allocate device space for vector B\n",
        "  cudaMalloc(&d_C, DSIZE*sizeof(float));  // allocate device space for vector C\n",
        "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
        "  // copy vector A to device:\n",
        "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  // copy vector B to device:\n",
        "  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
        "  //cuda processing sequence step 1 is complete\n",
        "  // int blocks = 1;  // modify this line for experimentation\n",
        "  int blocks = 160;\n",
        "  // int threads = 1; // modify this line for experimentation\n",
        "  int threads = 1024;\n",
        "  vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "  //cuda processing sequence step 2 is complete\n",
        "  // copy vector C from device to host:\n",
        "  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  //cuda processing sequence step 3 is complete\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
        "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
        "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "13dTrK4JLbJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c914abe8-d9a9-4b8c-88ed-a936c5a982ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda-training-series/exercises/hw3/vector_add_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw3/vector_add_hardik.cu -o cuda-training-series/exercises/hw3/vector_add_hardik\n",
        "./cuda-training-series/exercises/hw3/vector_add_hardik"
      ],
      "metadata": {
        "id": "cHKwymDqLhaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c281f39-6181-4f47-bea5-579dc27e157d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw3/vector_add_solution.cu"
      ],
      "metadata": {
        "id": "xN8faXZ7Lqnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21e023-c3fa-4589-8dc8-5deb6ad02b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const int DSIZE = 32*1048576;\n",
            "// vector add kernel: C = A + B\n",
            "__global__ void vadd(const float *A, const float *B, float *C, int ds){\n",
            "\n",
            "  for (int idx = threadIdx.x+blockDim.x*blockIdx.x; idx < ds; idx+=gridDim.x*blockDim.x)         // a grid-stride loop\n",
            "    C[idx] = A[idx] + B[idx]; // do the vector (element) add here\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
            "  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n",
            "  h_B = new float[DSIZE];\n",
            "  h_C = new float[DSIZE];\n",
            "  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n",
            "    h_A[i] = rand()/(float)RAND_MAX;\n",
            "    h_B[i] = rand()/(float)RAND_MAX;\n",
            "    h_C[i] = 0;}\n",
            "  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n",
            "  cudaMalloc(&d_B, DSIZE*sizeof(float));  // allocate device space for vector B\n",
            "  cudaMalloc(&d_C, DSIZE*sizeof(float));  // allocate device space for vector C\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "  // copy vector A to device:\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  // copy vector B to device:\n",
            "  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  int blocks = 1;  // modify this line for experimentation\n",
            "  int threads = 1; // modify this line for experimentation\n",
            "  vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector C from device to host:\n",
            "  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  printf(\"A[0] = %f\\n\", h_A[0]);\n",
            "  printf(\"B[0] = %f\\n\", h_B[0]);\n",
            "  printf(\"C[0] = %f\\n\", h_C[0]);\n",
            "  return 0;\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Profiling Experiments\n"
      ],
      "metadata": {
        "id": "YJ5eUmA9Xa8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2a. 1 block of 1 thread"
      ],
      "metadata": {
        "id": "5TlFLumGZsJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu cuda-training-series/exercises/hw3/vector_add_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV14B3FmW11d",
        "outputId": "b628a6e9-e19e-4cab-a781-37280d624ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 5448 (/content/cuda-training-series/exercises/hw3/vector_add_hardik)\n",
            "==PROF== Profiling \"vadd\" - 0: 0%..\n",
            "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
            "..50%....100% - 8 passes\n",
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n",
            "==PROF== Disconnected from process 5448\n",
            "[5448] vector_add_hardik@127.0.0.1\n",
            "  vadd(const float *, const float *, float *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- -------------\n",
            "    Metric Name               Metric Unit  Metric Value\n",
            "    ----------------------- ------------- -------------\n",
            "    DRAM Frequency          cycle/nsecond          5.00\n",
            "    SM Frequency            cycle/usecond        584.99\n",
            "    Elapsed Cycles                  cycle 3,300,469,889\n",
            "    Memory Throughput                   %          0.15\n",
            "    DRAM Throughput                     %          0.06\n",
            "    Duration                       second          5.64\n",
            "    L1/TEX Cache Throughput             %          6.10\n",
            "    L2 Cache Throughput                 %          0.05\n",
            "    SM Active Cycles                cycle 82,512,655.97\n",
            "    Compute (SM) Throughput             %          0.15\n",
            "    ----------------------- ------------- -------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                     1\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread               1\n",
            "    Waves Per SM                                                0.00\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 3.03%                                                                                      \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block          128\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block           32\n",
            "    Theoretical Active Warps per SM        warp           16\n",
            "    Theoretical Occupancy                     %           50\n",
            "    Achieved Occupancy                        %         3.12\n",
            "    Achieved Active Warps Per SM           warp            1\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 93.75%                                                                                     \n",
            "          This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM. This   \n",
            "          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory. The difference     \n",
            "          between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the result of warp       \n",
            "          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    \n",
            "          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu --section SpeedOfLight --section MemoryWorkloadAnalysis cuda-training-series/exercises/hw3/vector_add_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaBH0fdXZnyX",
        "outputId": "1acc9b63-1dc5-4d29-970d-5d3d361d011e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 6176 (/content/cuda-training-series/exercises/hw3/vector_add_hardik)\n",
            "==PROF== Profiling \"vadd\" - 0: 0%..\n",
            "==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.\n",
            "..50%....100% - 9 passes\n",
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n",
            "==PROF== Disconnected from process 6176\n",
            "[6176] vector_add_hardik@127.0.0.1\n",
            "  vadd(const float *, const float *, float *, int) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- -------------\n",
            "    Metric Name               Metric Unit  Metric Value\n",
            "    ----------------------- ------------- -------------\n",
            "    DRAM Frequency          cycle/nsecond          5.00\n",
            "    SM Frequency            cycle/usecond        584.99\n",
            "    Elapsed Cycles                  cycle 3,300,479,730\n",
            "    Memory Throughput                   %          0.15\n",
            "    DRAM Throughput                     %          0.06\n",
            "    Duration                       second          5.64\n",
            "    L1/TEX Cache Throughput             %          6.10\n",
            "    L2 Cache Throughput                 %          0.05\n",
            "    SM Active Cycles                cycle 82,512,209.22\n",
            "    Compute (SM) Throughput             %          0.15\n",
            "    ----------------------- ------------- -------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Memory Workload Analysis\n",
            "    ----------------- ------------ ------------\n",
            "    Metric Name        Metric Unit Metric Value\n",
            "    ----------------- ------------ ------------\n",
            "    Memory Throughput Mbyte/second       198.94\n",
            "    Mem Busy                     %         0.08\n",
            "    Max Bandwidth                %         0.15\n",
            "    L1/TEX Hit Rate              %        90.62\n",
            "    L2 Hit Rate                  %        78.97\n",
            "    Mem Pipes Busy               %         0.15\n",
            "    ----------------- ------------ ------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2b. 1 block of 1024 threads"
      ],
      "metadata": {
        "id": "uUVUqzl2aiYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu --section SpeedOfLight --section MemoryWorkloadAnalysis cuda-training-series/exercises/hw3/vector_add_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRBMyZZdah2G",
        "outputId": "4a7eaeb9-98ef-4ab5-dfe7-f6197a9a01bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 7273 (/content/cuda-training-series/exercises/hw3/vector_add_hardik)\n",
            "==PROF== Profiling \"vadd\" - 0: 0%....50%....100% - 9 passes\n",
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n",
            "==PROF== Disconnected from process 7273\n",
            "[7273] vector_add_hardik@127.0.0.1\n",
            "  vadd(const float *, const float *, float *, int) (1, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         5.00\n",
            "    SM Frequency            cycle/usecond       585.21\n",
            "    Elapsed Cycles                  cycle   13,547,994\n",
            "    Memory Throughput                   %         7.66\n",
            "    DRAM Throughput                     %         7.66\n",
            "    Duration                      msecond        23.15\n",
            "    L1/TEX Cache Throughput             %        61.95\n",
            "    L2 Cache Throughput                 %         1.99\n",
            "    SM Active Cycles                cycle   338,538.90\n",
            "    Compute (SM) Throughput             %         1.16\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Memory Workload Analysis\n",
            "    ----------------- ------------ ------------\n",
            "    Metric Name        Metric Unit Metric Value\n",
            "    ----------------- ------------ ------------\n",
            "    Memory Throughput Gbyte/second        24.50\n",
            "    Mem Busy                     %         1.99\n",
            "    Max Bandwidth                %         7.66\n",
            "    L1/TEX Hit Rate              %            0\n",
            "    L2 Hit Rate                  %        33.30\n",
            "    Mem Pipes Busy               %         1.16\n",
            "    ----------------- ------------ ------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2c. 160 blocks of 1024 threads"
      ],
      "metadata": {
        "id": "RABkzGh8a-Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu --section SpeedOfLight --section MemoryWorkloadAnalysis cuda-training-series/exercises/hw3/vector_add_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNLav7Yfa9wF",
        "outputId": "feb6bf0d-656a-4ad1-d7c3-30a618d0b7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 7858 (/content/cuda-training-series/exercises/hw3/vector_add_hardik)\n",
            "==PROF== Profiling \"vadd\" - 0: 0%....50%....100% - 9 passes\n",
            "A[0] = 0.840188\n",
            "B[0] = 0.394383\n",
            "C[0] = 1.234571\n",
            "==PROF== Disconnected from process 7858\n",
            "[7858] vector_add_hardik@127.0.0.1\n",
            "  vadd(const float *, const float *, float *, int) (160, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.97\n",
            "    SM Frequency            cycle/usecond       581.55\n",
            "    Elapsed Cycles                  cycle    1,047,612\n",
            "    Memory Throughput                   %        86.98\n",
            "    DRAM Throughput                     %        86.98\n",
            "    Duration                      msecond         1.80\n",
            "    L1/TEX Cache Throughput             %        28.79\n",
            "    L2 Cache Throughput                 %        25.68\n",
            "    SM Active Cycles                cycle 1,036,699.50\n",
            "    Compute (SM) Throughput             %        15.04\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Memory Workload Analysis\n",
            "    ----------------- ------------ ------------\n",
            "    Metric Name        Metric Unit Metric Value\n",
            "    ----------------- ------------ ------------\n",
            "    Memory Throughput Gbyte/second       276.46\n",
            "    Mem Busy                     %        25.68\n",
            "    Max Bandwidth                %        86.98\n",
            "    L1/TEX Hit Rate              %            0\n",
            "    L2 Hit Rate                  %        33.35\n",
            "    Mem Pipes Busy               %        15.04\n",
            "    ----------------- ------------ ------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW4"
      ],
      "metadata": {
        "id": "qLVZirZJ4B57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw4/readme.md"
      ],
      "metadata": {
        "id": "G6-9Fe844EWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59162460-cd01-4d47-f23d-8c5f43c41e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## **1. Matrix Row/Column Sums**\n",
            "\n",
            "Your first task is to create a simple matrix row and column sum application in CUDA. The code skeleton is already given to you in *matrix_sums.cu*. Edit that file, paying attention to the FIXME locations, so that the output when run is like this:\n",
            "\n",
            "```\n",
            "row sums correct!\n",
            "column sums correct!\n",
            "```\n",
            "\n",
            "After editing the code, compile it using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o matrix_sums matrix_sums.cu\n",
            "```\n",
            "\n",
            "The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. *nvcc* is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n",
            "\n",
            "To run your code, we will use an LSF command:\n",
            "\n",
            "```\n",
            "bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./matrix_sums\n",
            "```\n",
            "\n",
            "Alternatively, you may want to create an alias for your bsub command in order to make subsequent runs easier:\n",
            "\n",
            "```\n",
            "alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n",
            "lsfrun ./matrix_sums\n",
            "```\n",
            "\n",
            "To run your code at NERSC on Cori, we can use Slurm:\n",
            "\n",
            "```\n",
            "module load esslurm\n",
            "srun -C gpu -N 1 -n 1 -t 10 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10 ./matrix_sums\n",
            "```\n",
            "\n",
            "Allocation `m3502` is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes (such as m1759), you may use that instead.\n",
            "\n",
            "If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active (this is recommended if there are enough available nodes):\n",
            "\n",
            "```\n",
            "salloc -C gpu -N 1 -t 60 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10\n",
            "srun -n 1 ./matrix_sums\n",
            "```\n",
            "\n",
            "Note that you only need to `module load esslurm` once per login session; this is what enables you to submit to the Cori GPU nodes.\n",
            "\n",
            "\n",
            "If you have trouble, you can look at *matrix_sums_solution.cu* for a complete example.\n",
            "\n",
            "## **2. Profiling**\n",
            "\n",
            "We'll introduce something new: the profiler (in this case, Nsight Compute). We'll use the profiler first to time the kernel execution times, and then to gather some \"metric\" information that will possibly shed light on our observations.\n",
            "\n",
            "It's necessary to complete task 1 first. Next, load the Nsight Compute module:\n",
            "```\n",
            "module load nsight-compute\n",
            "```\n",
            "\n",
            "Then, launch Nsight as follows:\n",
            "(you may want to make your terminal session wide enough to make the output easy to read)\n",
            "\n",
            "```\n",
            "lsfrun nv-nsight-cu-cli ./matrix_sums\n",
            "```\n",
            "\n",
            "What does the output tell you?\n",
            "Can you locate the lines that identify the kernel durations?\n",
            "Are the kernel durations the same or different?\n",
            "Would you expect them to be the same or different?\n",
            "\n",
            "\n",
            "Next, launch *Nsight* as follows:\n",
            "\n",
            "```\n",
            "lsfrun nv-nsight-cu-cli --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum ./matrix_sums\n",
            "```\n",
            "\n",
            "Our goal is to measure the global memory load efficiency of our kernels. In this case we have asked for two metrics: \"*l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum*\" (the number of global memory load requests) and \"*l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum*\" (the number of sectors requested for global loads). This first metric above represents the denominator (requests) of the desired measurement (transactions per request) and the second metric represents the numerator (transactions). Dividing these numbers will give us the number of transactions per request. \n",
            "\n",
            "What similarities or differences do you notice between the *row_sum* and *column_sum* kernels?\n",
            "Do the kernels (*row_sum*, *column_sum*) have the same or different efficiencies?\n",
            "Why?\n",
            "How does this correspond to the observed kernel execution times for the first profiling run?\n",
            "\n",
            "Can we improve this?  (Stay tuned for the next CUDA training session.)\n",
            "\n",
            "Here is a useful blog to help you get familiar with Nsight Compute: https://devblogs.nvidia.com/using-nsight-compute-to-inspect-your-kernels/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Matrix Row/Column Sums"
      ],
      "metadata": {
        "id": "O8iTGTP34O8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw4/matrix_sums.cu"
      ],
      "metadata": {
        "id": "uAPKw5vK4Rms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9985d362-6a6a-47c3-b7a0-07d1e5f53f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "const size_t DSIZE = 16384;      // matrix side dimension\n",
            "const int block_size = 256;  // CUDA maximum is 1024\n",
            "\n",
            "// matrix row-sum kernel\n",
            "__global__ void row_sums(const float *A, float *sums, size_t ds){\n",
            "\n",
            "  int idx = FIXME // create typical 1D thread index from built-in variables\n",
            "  if (idx < ds){\n",
            "    float sum = 0.0f;\n",
            "    for (size_t i = 0; i < ds; i++)\n",
            "      sum += A[FIXME]         // write a for loop that will cause the thread to iterate across a row, keeeping a running sum, and write the result to sums\n",
            "    sums[idx] = sum;\n",
            "}}\n",
            "\n",
            "// matrix column-sum kernel\n",
            "__global__ void column_sums(const float *A, float *sums, size_t ds){\n",
            "\n",
            "  int idx = FIXME // create typical 1D thread index from built-in variables\n",
            "  if (idx < ds){\n",
            "    float sum = 0.0f;\n",
            "    for (size_t i = 0; i < ds; i++)\n",
            "      sum += A[FIXME]         // write a for loop that will cause the thread to iterate down a column, keeeping a running sum, and write the result to sums\n",
            "    sums[idx] = sum;\n",
            "}}\n",
            "\n",
            "bool validate(float *data, size_t sz){\n",
            "  for (size_t i = 0; i < sz; i++)\n",
            "    if (data[i] != (float)sz) {printf(\"results mismatch at %lu, was: %f, should be: %f\\n\", i, data[i], (float)sz); return false;}\n",
            "    return true;\n",
            "}\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_sums, *d_A, *d_sums;\n",
            "  h_A = new float[DSIZE*DSIZE];  // allocate space for data in host memory\n",
            "  h_sums = new float[DSIZE]();\n",
            "    \n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++)  // initialize matrix in host memory\n",
            "    h_A[i] = 1.0f;\n",
            "    \n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));  // allocate device space for A\n",
            "  FIXME // allocate device space for vector d_sums\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "    \n",
            "  // copy matrix A to device:\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "    \n",
            "  //cuda processing sequence step 1 is complete\n",
            "  row_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "    \n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "    \n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "    \n",
            "  if (!validate(h_sums, DSIZE)) return -1; \n",
            "  printf(\"row sums correct!\\n\");\n",
            "    \n",
            "  cudaMemset(d_sums, 0, DSIZE*sizeof(float));\n",
            "    \n",
            "  column_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "    \n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "    \n",
            "  if (!validate(h_sums, DSIZE)) return -1; \n",
            "  printf(\"column sums correct!\\n\");\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw4/matrix_sums_hardik.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "// error checking macro\n",
        "#define cudaCheckErrors(msg) \\\n",
        "    do { \\\n",
        "        cudaError_t __err = cudaGetLastError(); \\\n",
        "        if (__err != cudaSuccess) { \\\n",
        "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "                msg, cudaGetErrorString(__err), \\\n",
        "                __FILE__, __LINE__); \\\n",
        "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "            exit(1); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "const size_t DSIZE = 16384;      // matrix side dimension\n",
        "const int block_size = 256;  // CUDA maximum is 1024\n",
        "\n",
        "// matrix row-sum kernel\n",
        "__global__ void row_sums(const float *A, float *sums, size_t ds){\n",
        "\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x; // create typical 1D thread index from built-in variables\n",
        "  if (idx < ds){\n",
        "    float sum = 0.0f;\n",
        "    for (size_t i = 0; i < ds; i++)\n",
        "      sum += A[idx * ds + i];         // write a for loop that will cause the thread to iterate across a row, keeeping a running sum, and write the result to sums\n",
        "    sums[idx] = sum;\n",
        "}}\n",
        "\n",
        "// matrix column-sum kernel\n",
        "__global__ void column_sums(const float *A, float *sums, size_t ds){\n",
        "\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x; // create typical 1D thread index from built-in variables\n",
        "  if (idx < ds){\n",
        "    float sum = 0.0f;\n",
        "    for (size_t i = 0; i < ds; i++)\n",
        "      sum += A[idx + i * ds];         // write a for loop that will cause the thread to iterate down a column, keeeping a running sum, and write the result to sums\n",
        "    sums[idx] = sum;\n",
        "}}\n",
        "\n",
        "bool validate(float *data, size_t sz){\n",
        "  for (size_t i = 0; i < sz; i++)\n",
        "    if (data[i] != (float)sz) {printf(\"results mismatch at %lu, was: %f, should be: %f\\n\", i, data[i], (float)sz); return false;}\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  float *h_A, *h_sums, *d_A, *d_sums;\n",
        "  h_A = new float[DSIZE*DSIZE];  // allocate space for data in host memory\n",
        "  h_sums = new float[DSIZE]();\n",
        "\n",
        "  for (int i = 0; i < DSIZE*DSIZE; i++)  // initialize matrix in host memory\n",
        "    h_A[i] = 1.0f;\n",
        "\n",
        "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));  // allocate device space for A\n",
        "  cudaMalloc(&d_sums, DSIZE*sizeof(float)); // allocate device space for vector d_sums\n",
        "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
        "\n",
        "  // copy matrix A to device:\n",
        "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
        "\n",
        "  //cuda processing sequence step 1 is complete\n",
        "  row_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "  //cuda processing sequence step 2 is complete\n",
        "\n",
        "  // copy vector sums from device to host:\n",
        "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  //cuda processing sequence step 3 is complete\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "\n",
        "  if (!validate(h_sums, DSIZE)) return -1;\n",
        "  printf(\"row sums correct!\\n\");\n",
        "\n",
        "  cudaMemset(d_sums, 0, DSIZE*sizeof(float));\n",
        "\n",
        "  column_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
        "  cudaCheckErrors(\"kernel launch failure\");\n",
        "  //cuda processing sequence step 2 is complete\n",
        "\n",
        "  // copy vector sums from device to host:\n",
        "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  //cuda processing sequence step 3 is complete\n",
        "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
        "\n",
        "  if (!validate(h_sums, DSIZE)) return -1;\n",
        "  printf(\"column sums correct!\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "eUNH_Zgr4uoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8710b32-b3d7-4432-cda8-c9ca4a12e0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda-training-series/exercises/hw4/matrix_sums_hardik.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw4/matrix_sums_hardik.cu -o cuda-training-series/exercises/hw4/matrix_sums_hardik\n",
        "./cuda-training-series/exercises/hw4/matrix_sums_hardik"
      ],
      "metadata": {
        "id": "k2wDLpSc4xhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458c3a0d-bb2c-4c87-fe9a-43a7c0410e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row sums correct!\n",
            "column sums correct!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw4/matrix_sums_solution.cu"
      ],
      "metadata": {
        "id": "zuH8-wTO5Ihn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501237f4-7183-4f18-e83b-adc8f8292f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const size_t DSIZE = 16384;      // matrix side dimension\n",
            "const int block_size = 256;  // CUDA maximum is 1024\n",
            "\n",
            "// matrix row-sum kernel\n",
            "__global__ void row_sums(const float *A, float *sums, size_t ds){\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create typical 1D thread index from built-in variables\n",
            "  if (idx < ds){\n",
            "    float sum = 0.0f;\n",
            "    for (size_t i = 0; i < ds; i++)\n",
            "      sum += A[idx*ds+i];         // write a for loop that will cause the thread to iterate across a row, keeeping a running sum, and write the result to sums\n",
            "    sums[idx] = sum;\n",
            "}}\n",
            "\n",
            "// matrix column-sum kernel\n",
            "__global__ void column_sums(const float *A, float *sums, size_t ds){\n",
            "  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create typical 1D thread index from built-in variables\n",
            "  if (idx < ds){\n",
            "    float sum = 0.0f;\n",
            "    for (size_t i = 0; i < ds; i++)\n",
            "      sum += A[idx+ds*i];         // write a for loop that will cause the thread to iterate down a column, keeeping a running sum, and write the result to sums\n",
            "    sums[idx] = sum;\n",
            "}}\n",
            "\n",
            "bool validate(float *data, size_t sz){\n",
            "  for (size_t i = 0; i < sz; i++)\n",
            "    if (data[i] != (float)sz) {printf(\"results mismatch at %lu, was: %f, should be: %f\\n\", i, data[i], (float)sz); return false;}\n",
            "    return true;\n",
            "}\n",
            "\n",
            "int main(){\n",
            "  float *h_A, *h_sums, *d_A, *d_sums;\n",
            "  h_A = new float[DSIZE*DSIZE];  // allocate space for data in host memory\n",
            "  h_sums = new float[DSIZE]();\n",
            "    \n",
            "  for (int i = 0; i < DSIZE*DSIZE; i++)  // initialize matrix in host memory\n",
            "    h_A[i] = 1.0f;\n",
            "    \n",
            "  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));  // allocate device space for A\n",
            "  cudaMalloc(&d_sums, DSIZE*sizeof(float));  // allocate device space for vector d_sums\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "    \n",
            "  // copy matrix A to device:\n",
            "  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "    \n",
            "  row_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "    \n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "    \n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  if (!validate(h_sums, DSIZE)) return -1; \n",
            "  printf(\"row sums correct!\\n\");\n",
            "    \n",
            "  cudaMemset(d_sums, 0, DSIZE*sizeof(float));\n",
            "    \n",
            "  column_sums<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_sums, DSIZE);\n",
            "  cudaCheckErrors(\"kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "    \n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sums, d_sums, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "    \n",
            "  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  if (!validate(h_sums, DSIZE)) return -1; \n",
            "  printf(\"column sums correct!\\n\");\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Profiling"
      ],
      "metadata": {
        "id": "mLMFjYhY4YjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu cuda-training-series/exercises/hw4/matrix_sums_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibcyeym2E5nS",
        "outputId": "7063bff2-27ad-4636-fd5f-d2a7c8a1d2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 4460 (/content/cuda-training-series/exercises/hw4/matrix_sums_hardik)\n",
            "==PROF== Profiling \"row_sums\" - 0: 0%....50%....100% - 8 passes\n",
            "row sums correct!\n",
            "==PROF== Profiling \"column_sums\" - 1: 0%....50%....100% - 8 passes\n",
            "column sums correct!\n",
            "==PROF== Disconnected from process 4460\n",
            "[4460] matrix_sums_hardik@127.0.0.1\n",
            "  row_sums(const float *, float *, unsigned long) (64, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- -------------\n",
            "    Metric Name               Metric Unit  Metric Value\n",
            "    ----------------------- ------------- -------------\n",
            "    DRAM Frequency          cycle/nsecond          5.00\n",
            "    SM Frequency            cycle/usecond        585.32\n",
            "    Elapsed Cycles                  cycle    16,825,602\n",
            "    Memory Throughput                   %         39.89\n",
            "    DRAM Throughput                     %         24.12\n",
            "    Duration                      msecond         28.75\n",
            "    L1/TEX Cache Throughput             %         79.78\n",
            "    L2 Cache Throughput                 %          4.89\n",
            "    SM Active Cycles                cycle 11,830,073.78\n",
            "    Compute (SM) Throughput             %          2.49\n",
            "    ----------------------- ------------- -------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                     64\n",
            "    Registers Per Thread             register/thread              20\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread          16,384\n",
            "    Waves Per SM                                                0.40\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the \n",
            "          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   \n",
            "          hardware busy.                                                                                                \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        36.00\n",
            "    Achieved Active Warps Per SM           warp        11.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 64%                                                                                        \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (36.0%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  column_sums(const float *, float *, unsigned long) (64, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         5.01\n",
            "    SM Frequency            cycle/usecond       586.34\n",
            "    Elapsed Cycles                  cycle    2,905,134\n",
            "    Memory Throughput                   %        96.06\n",
            "    DRAM Throughput                     %        96.06\n",
            "    Duration                      msecond         4.95\n",
            "    L1/TEX Cache Throughput             %        29.85\n",
            "    L2 Cache Throughput                 %        24.70\n",
            "    SM Active Cycles                cycle 2,810,217.98\n",
            "    Compute (SM) Throughput             %        14.44\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                     64\n",
            "    Registers Per Thread             register/thread              20\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread          16,384\n",
            "    Waves Per SM                                                0.40\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the \n",
            "          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   \n",
            "          hardware busy.                                                                                                \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        40.08\n",
            "    Achieved Active Warps Per SM           warp        12.83\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 59.92%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (40.1%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu --metrics l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum cuda-training-series/exercises/hw4/matrix_sums_hardik"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIxnrAOTFdbH",
        "outputId": "6ab8341e-d2fe-4d31-c018-421f267f2c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 5086 (/content/cuda-training-series/exercises/hw4/matrix_sums_hardik)\n",
            "==PROF== Profiling \"row_sums\" - 0: 0%....50%....100% - 3 passes\n",
            "row sums correct!\n",
            "==PROF== Profiling \"column_sums\" - 1: 0%....50%....100% - 3 passes\n",
            "column sums correct!\n",
            "==PROF== Disconnected from process 5086\n",
            "[5086] matrix_sums_hardik@127.0.0.1\n",
            "  row_sums(const float *, float *, unsigned long) (64, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "    Metric Name                                     Metric Unit Metric Value\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "    l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum     request    8,388,608\n",
            "    l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum       sector  268,361,891\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "\n",
            "  column_sums(const float *, float *, unsigned long) (64, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "    Metric Name                                     Metric Unit Metric Value\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "    l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum     request    8,388,608\n",
            "    l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum       sector   33,554,432\n",
            "    ----------------------------------------------- ----------- ------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW5"
      ],
      "metadata": {
        "id": "eYgLu6KFOicA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/readme.md"
      ],
      "metadata": {
        "id": "c76AJCG0Oj_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b36492-67f8-4d5c-d56a-33b4b4e74499"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## **1. Comparing Reductions**\n",
            "\n",
            "For your first task, the code is already written for you. We will compare 3 of the reductions given during the presentation: the naive atomic-only reduction, the classical parallel reduction with atomic finish, and the warp shuffle reduction (with atomic finish).\n",
            "\n",
            "Compile it using the following:\n",
            "\n",
            "```\n",
            "module load cuda\n",
            "nvcc -o reductions reductions.cu\n",
            "```\n",
            "\n",
            "The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. *nvcc* is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++. Let's also load the Nsight Compute module:\n",
            "\n",
            "```\n",
            "module load nsight-compute\n",
            "```\n",
            "\n",
            "To run your code, we will use an LSF command:\n",
            "\n",
            "```\n",
            "bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 nv-nsight-cu-cli ./reductions\n",
            "```\n",
            "\n",
            "Alternatively, you may want to create an alias for your *bsub* command in order to make subsequent runs easier:\n",
            "\n",
            "```\n",
            "alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n",
            "lsfrun nv-nsight-cu-cli ./reductions\n",
            "```\n",
            "\n",
            "To run your code at NERSC on Cori, we can use Slurm:\n",
            "\n",
            "```\n",
            "module load esslurm\n",
            "srun -C gpu -N 1 -n 1 -t 10 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10 ./reductions\n",
            "```\n",
            "\n",
            "Allocation `m3502` is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes (such as m1759), you may use that instead.\n",
            "\n",
            "If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active (this is recommended if there are enough available nodes):\n",
            "\n",
            "```\n",
            "salloc -C gpu -N 1 -t 60 -A m3502 --reservation cuda_training --gres=gpu:1 -c 10\n",
            "srun -n 1 ./reductions\n",
            "```\n",
            "\n",
            "Note that you only need to `module load esslurm` once per login session; this is what enables you to submit to the Cori GPU nodes.\n",
            "\n",
            "This will run the code with the profiling in its most basic mode, which is sufficient. We want to compare kernel execution times. What do you notice about kernel execution times? Probably, you won't see much difference between the parallel reduction with atomics and the warp shuffle with atomics kernel. Can you theorize why this may be? Our objective with these will be to approach theoretical limits. The theoretical limit for a typical reduction would be determined by the memory bandwidth of the GPU. To calculate the attained memory bandwidth of this kernel, divide the total data size in bytes (use N from the code in your calculation) by the execution time (which you can get from the profiler). How does this number compare to the memory bandwidth of the GPU you are running on? (You could run bandwidthTest sample code to get a proxy/estimate).\n",
            "\n",
            "Now edit the code to change *N* from ~8M to 163840 (=640*256)\n",
            "\n",
            "Recompile and re-run the code with profiling. Is there a bigger percentage difference between the execution time of the reduce_a and reduce_ws kernel? Why might this be?\n",
            "\n",
            "Bonus: edit the code to change *N* from ~8M to ~32M.  recompile and run.  What happened? Why?\n",
            "\n",
            "## **2. Create a different reduction (besides sum)**\n",
            "\n",
            "For this exercise, you are given a fully-functional sum-reduction code, similar to the code used for exercise 1 above, except that we will use the 2-stage reduction method without atomic finish. If you wish you can compile and run it as-is to see how it works. Your task is to modify it (*only the kernel*) so that it creates a proper max-finding reduction. That means that the kernel should report the maximum value in the data set, rather than the sum of the data set. You are expected to use a similar parallel-sweep-reduction technique. If you need help, refer to the solution.\n",
            "\n",
            "```\n",
            "nvcc -o max_reduction max_reduction.cu\n",
            "lsfrun ./max_reduction\n",
            "```\n",
            "\n",
            "## **3. Revisit row_sums from hw4**\n",
            "\n",
            "For this exercise, start with the *matrix_sums.cu* code from hw4. As you may recall, the *row_sums* kernel was reading the same data set as the *column_sums* kernel, but running noticeably slower. We now have some ideas how to fix it. See if you can implement a reduction-per-row, to allow the row-sum kernel to approach the performance of the column sum kernel. There are probably several ways to tackle this problem. To see one approach, refer to the solution.\n",
            "\n",
            "You can start just by compiling the code as-is and running the profiler to remind yourself of the performance (discrepancy).\n",
            "\n",
            "Compile the code and profile it using Nsight Compute:\n",
            "\n",
            "```\n",
            "nvcc -o matrix_sums matrix_sums.cu\n",
            "lsfrun nv-nsight-cu-cli ./matrix_sums\n",
            "```\n",
            "\n",
            "Remember from the previous session our top 2 CUDA optimization priorities: lots of threads and efficient use of the memory subsystem. The original row_sums kernel definitely misses the mark for the memory objective. What we've learned about reductions should guide you. There are probably several ways to tackle this:\n",
            "\n",
            " - Write a straightforward parallel reduction, run it on a row, and use a for-loop to loop the kernel over all rows\n",
            " - Assign a warp to each row, to perform everything in one kernel call\n",
            " - Assign a threadblock to each row, to perform everything in one kernel call\n",
            " - ??\n",
            "\n",
            "Since the (given) solution may be somewhat unusual, I'll give some hints here if needed:\n",
            "\n",
            " - The chosen strategy will be to assign one block per row\n",
            " - We must modify the kernel launch to launch exactly as many blocks as we have rows\n",
            " - The kernel can be adapted from the reduction kernel (atomic is not needed here) from the reduce kernel code in exercise 1 above.\n",
            " - Since we are assigning one block per row, we will cause each block to perform a block-striding loop, to traverse the row.  This is conceptually similar to a grid striding loop, except each block is striding individually, one per row.  Refresh your memory of the grid-stride loop, and see if you can work this out.\n",
            " - With the block-stride loop, you'll need to think carefully about indexing\n",
            "\n",
            "After you have completed the work and are getting a successful result, profile the code again to see if the performance of the row_sums kernel has improved:\n",
            "\n",
            "```\n",
            "nvcc -o matrix_sums matrix_sums.cu\n",
            "lsfrun nv-nsight-cu-cli ./matrix_sums\n",
            "```\n",
            "\n",
            "Your actual performance here (compared to the fairly efficient column_sums kernel) will probably depend quite a bit on the algorithm/method you choose.  See if you can theorize how the various choices may affect efficiency or optimality. If you end up with a solution where the row_sums kernel actually runs faster than the column_sums kernel, see if you can theorize why this may be. Remember the two CUDA optimization priorities, and use these to guide your thinking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Comparing Reductions"
      ],
      "metadata": {
        "id": "4V3C9KmhPPmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/reductions.cu"
      ],
      "metadata": {
        "id": "CI4XBiMCPO1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb121dca-8602-4bcc-f9db-a5c8efd34ff0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// error checking macro\n",
            "#define cudaCheckErrors(msg) \\\n",
            "    do { \\\n",
            "        cudaError_t __err = cudaGetLastError(); \\\n",
            "        if (__err != cudaSuccess) { \\\n",
            "            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
            "                msg, cudaGetErrorString(__err), \\\n",
            "                __FILE__, __LINE__); \\\n",
            "            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
            "            exit(1); \\\n",
            "        } \\\n",
            "    } while (0)\n",
            "\n",
            "\n",
            "const size_t N = 8ULL*1024ULL*1024ULL;  // data size\n",
            "//const size_t N = 256*640; // data size\n",
            "const int BLOCK_SIZE = 256;  // CUDA maximum is 1024\n",
            "// naive atomic reduction kernel\n",
            "__global__ void atomic_red(const float *gdata, float *out){\n",
            "  size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
            "  if (idx < N) atomicAdd(out, gdata[idx]);\n",
            "}\n",
            "\n",
            "__global__ void reduce(float *gdata, float *out){\n",
            "     __shared__ float sdata[BLOCK_SIZE];\n",
            "     int tid = threadIdx.x;\n",
            "     sdata[tid] = 0.0f;\n",
            "     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
            "\n",
            "     while (idx < N) {  // grid stride loop to load data\n",
            "        sdata[tid] += gdata[idx];\n",
            "        idx += gridDim.x*blockDim.x;  \n",
            "        }\n",
            "\n",
            "     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
            "        __syncthreads();\n",
            "        if (tid < s)  // parallel sweep reduction\n",
            "            sdata[tid] += sdata[tid + s];\n",
            "        }\n",
            "     if (tid == 0) out[blockIdx.x] = sdata[0];\n",
            "  }\n",
            "\n",
            " __global__ void reduce_a(float *gdata, float *out){\n",
            "     __shared__ float sdata[BLOCK_SIZE];\n",
            "     int tid = threadIdx.x;\n",
            "     sdata[tid] = 0.0f;\n",
            "     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
            "\n",
            "     while (idx < N) {  // grid stride loop to load data\n",
            "        sdata[tid] += gdata[idx];\n",
            "        idx += gridDim.x*blockDim.x;  \n",
            "        }\n",
            "\n",
            "     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
            "        __syncthreads();\n",
            "        if (tid < s)  // parallel sweep reduction\n",
            "            sdata[tid] += sdata[tid + s];\n",
            "        }\n",
            "     if (tid == 0) atomicAdd(out, sdata[0]);\n",
            "  }\n",
            "\n",
            "\n",
            "__global__ void reduce_ws(float *gdata, float *out){\n",
            "     __shared__ float sdata[32];\n",
            "     int tid = threadIdx.x;\n",
            "     int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
            "     float val = 0.0f;\n",
            "     unsigned mask = 0xFFFFFFFFU;\n",
            "     int lane = threadIdx.x % warpSize;\n",
            "     int warpID = threadIdx.x / warpSize;\n",
            "     while (idx < N) {  // grid stride loop to load \n",
            "        val += gdata[idx];\n",
            "        idx += gridDim.x*blockDim.x;  \n",
            "        }\n",
            "\n",
            " // 1st warp-shuffle reduction\n",
            "    for (int offset = warpSize/2; offset > 0; offset >>= 1) \n",
            "       val += __shfl_down_sync(mask, val, offset);\n",
            "    if (lane == 0) sdata[warpID] = val;\n",
            "   __syncthreads(); // put warp results in shared mem\n",
            "\n",
            "// hereafter, just warp 0\n",
            "    if (warpID == 0){\n",
            " // reload val from shared mem if warp existed\n",
            "       val = (tid < blockDim.x/warpSize)?sdata[lane]:0;\n",
            "\n",
            " // final warp-shuffle reduction\n",
            "       for (int offset = warpSize/2; offset > 0; offset >>= 1) \n",
            "          val += __shfl_down_sync(mask, val, offset);\n",
            "\n",
            "       if  (tid == 0) atomicAdd(out, val);\n",
            "     }\n",
            "  }\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "int main(){\n",
            "\n",
            "  float *h_A, *h_sum, *d_A, *d_sum;\n",
            "  h_A = new float[N];  // allocate space for data in host memory\n",
            "  h_sum = new float;\n",
            "  for (int i = 0; i < N; i++)  // initialize matrix in host memory\n",
            "    h_A[i] = 1.0f;\n",
            "  cudaMalloc(&d_A, N*sizeof(float));  // allocate device space for A\n",
            "  cudaMalloc(&d_sum, sizeof(float));  // allocate device space for sum\n",
            "  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n",
            "  // copy matrix A to device:\n",
            "  cudaMemcpy(d_A, h_A, N*sizeof(float), cudaMemcpyHostToDevice);\n",
            "  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n",
            "  cudaMemset(d_sum, 0, sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMemset failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  atomic_red<<<(N+BLOCK_SIZE-1)/BLOCK_SIZE, BLOCK_SIZE>>>(d_A, d_sum);\n",
            "  cudaCheckErrors(\"atomic reduction kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"atomic reduction kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  if (*h_sum != (float)N) {printf(\"atomic sum reduction incorrect!\\n\"); return -1;}\n",
            "  printf(\"atomic sum reduction correct!\\n\");\n",
            "  const int blocks = 640;\n",
            "  cudaMemset(d_sum, 0, sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMemset failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  reduce_a<<<blocks, BLOCK_SIZE>>>(d_A, d_sum);\n",
            "  cudaCheckErrors(\"reduction w/atomic kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  if (*h_sum != (float)N) {printf(\"reduction w/atomic sum incorrect!\\n\"); return -1;}\n",
            "  printf(\"reduction w/atomic sum correct!\\n\");\n",
            "  cudaMemset(d_sum, 0, sizeof(float));\n",
            "  cudaCheckErrors(\"cudaMemset failure\");\n",
            "  //cuda processing sequence step 1 is complete\n",
            "  reduce_ws<<<blocks, BLOCK_SIZE>>>(d_A, d_sum);\n",
            "  cudaCheckErrors(\"reduction warp shuffle kernel launch failure\");\n",
            "  //cuda processing sequence step 2 is complete\n",
            "  // copy vector sums from device to host:\n",
            "  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
            "  //cuda processing sequence step 3 is complete\n",
            "  cudaCheckErrors(\"reduction warp shuffle kernel execution failure or cudaMemcpy H2D failure\");\n",
            "  if (*h_sum != (float)N) {printf(\"reduction warp shuffle sum incorrect!\\n\"); return -1;}\n",
            "  printf(\"reduction warp shuffle sum correct!\\n\");\n",
            "  return 0;\n",
            "}\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw5/reductions.cu -o cuda-training-series/exercises/hw5/reductions\n",
        "./cuda-training-series/exercises/hw5/reductions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89pmwXk3SsHF",
        "outputId": "0287704c-9026-40b9-9189-025e9001274a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "atomic sum reduction correct!\n",
            "reduction w/atomic sum correct!\n",
            "reduction warp shuffle sum correct!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "ncu cuda-training-series/exercises/hw5/reductions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPLPecjFTGks",
        "outputId": "40793082-051b-4d2e-811b-8ef84e63ab59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 3593 (/content/cuda-training-series/exercises/hw5/reductions)\n",
            "==PROF== Profiling \"atomic_red\" - 0: 0%....50%....100% - 8 passes\n",
            "atomic sum reduction correct!\n",
            "==PROF== Profiling \"reduce_a(float *, float *)\" - 1: 0%....50%....100% - 8 passes\n",
            "reduction w/atomic sum correct!\n",
            "==PROF== Profiling \"reduce_ws(float *, float *)\" - 2: 0%....50%....100% - 8 passes\n",
            "reduction warp shuffle sum correct!\n",
            "==PROF== Disconnected from process 3593\n",
            "[3593] reductions@127.0.0.1\n",
            "  atomic_red(const float *, float *) (32768, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- -------------\n",
            "    Metric Name               Metric Unit  Metric Value\n",
            "    ----------------------- ------------- -------------\n",
            "    DRAM Frequency          cycle/nsecond          5.00\n",
            "    SM Frequency            cycle/usecond        585.00\n",
            "    Elapsed Cycles                  cycle    17,220,634\n",
            "    Memory Throughput                   %          2.06\n",
            "    DRAM Throughput                     %          0.50\n",
            "    Duration                      msecond         29.44\n",
            "    L1/TEX Cache Throughput             %          2.59\n",
            "    L2 Cache Throughput                 %          2.06\n",
            "    SM Active Cycles                cycle 17,209,366.57\n",
            "    Compute (SM) Throughput             %          1.22\n",
            "    ----------------------- ------------- -------------\n",
            "\n",
            "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                 32,768\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread       8,388,608\n",
            "    Waves Per SM                                              204.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        87.63\n",
            "    Achieved Active Warps Per SM           warp        28.04\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 12.37%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (87.6%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  reduce_a(float *, float *) (640, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.81\n",
            "    SM Frequency            cycle/usecond       563.22\n",
            "    Elapsed Cycles                  cycle       95,244\n",
            "    Memory Throughput                   %        80.81\n",
            "    DRAM Throughput                     %        80.81\n",
            "    Duration                      usecond       169.09\n",
            "    L1/TEX Cache Throughput             %        28.54\n",
            "    L2 Cache Throughput                 %        23.60\n",
            "    SM Active Cycles                cycle    91,864.38\n",
            "    Compute (SM) Throughput             %        21.09\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                    640\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block       Kbyte/block            1.02\n",
            "    Threads                                   thread         163,840\n",
            "    Waves Per SM                                                   4\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           32\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        98.14\n",
            "    Achieved Active Warps Per SM           warp        31.41\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n",
            "  reduce_ws(float *, float *) (640, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.81\n",
            "    SM Frequency            cycle/usecond       563.66\n",
            "    Elapsed Cycles                  cycle       94,598\n",
            "    Memory Throughput                   %        81.47\n",
            "    DRAM Throughput                     %        81.47\n",
            "    Duration                      usecond       167.81\n",
            "    L1/TEX Cache Throughput             %        28.91\n",
            "    L2 Cache Throughput                 %        23.76\n",
            "    SM Active Cycles                cycle    90,678.10\n",
            "    Compute (SM) Throughput             %        18.48\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                    640\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block             128\n",
            "    Threads                                   thread         163,840\n",
            "    Waves Per SM                                                   4\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block          128\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        96.64\n",
            "    Achieved Active Warps Per SM           warp        30.93\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create a different reduction (besides sum)"
      ],
      "metadata": {
        "id": "pgdryo91PRCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/max_reduction.cu"
      ],
      "metadata": {
        "id": "DubWbmwTPuvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw5/max_reduction_hardik.cu"
      ],
      "metadata": {
        "id": "4fCKUS1SQDES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw5/max_reduction_hardik.cu -o cuda-training-series/exercises/hw5/max_reduction_hardik\n",
        "./cuda-training-series/exercises/hw5/max_reduction_hardik"
      ],
      "metadata": {
        "id": "MoOM4_p1QM4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/max_reduction_solution.cu"
      ],
      "metadata": {
        "id": "Uwyzf75kQWHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Revisit row_sums from hw4"
      ],
      "metadata": {
        "id": "3wzDMhd1PZgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/matrix_sums.cu"
      ],
      "metadata": {
        "id": "dwFsqvAaQaBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda-training-series/exercises/hw5/matrix_sums_hardik.cu"
      ],
      "metadata": {
        "id": "p0jcxhvxQaJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc cuda-training-series/exercises/hw5/matrix_sums_hardik.cu -o cuda-training-series/exercises/hw5/matrix_sums_hardik\n",
        "./cuda-training-series/exercises/hw5/matrix_sums_hardik"
      ],
      "metadata": {
        "id": "NgnWGTdQQaUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cuda-training-series/exercises/hw5/matrix_sums_solution.cu"
      ],
      "metadata": {
        "id": "Uvj6vzB8Qadb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* [An Even Easier Introduction to CUDA.ipynb](https://colab.research.google.com/github/NVDLI/notebooks/blob/master/even-easier-cuda/An_Even_Easier_Introduction_to_CUDA.ipynb)"
      ],
      "metadata": {
        "id": "Ht4vUg4aFZKM"
      }
    }
  ]
}